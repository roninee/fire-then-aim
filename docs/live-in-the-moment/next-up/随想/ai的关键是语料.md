## AI 的关键是语料

> 是 OpenAI 的工程师 James Betker 的观点。他是 AI 专家，著名"文生图"模型 DALL-E 的第一作者。https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/
>
> 他的结论是，模型的差异其实不是关键，决定性的是你的训练材料。只要有**更多更好的语料，不管用什么模型，都会得到差不多的结果。**
>
> 初次听到，你可能觉得，这个结论难以置信，但是仔细想想，你会发现它很可能是对的。
>
> 因为不管什么 AI 模型，最终目标只有一个，就是模仿人类的思维。语料体现的正是人类思维，同一份语料，不管你拿什么规则去分析，**最后得到的结果应该是一样的，**因为它包含的人类思维是不变的。
>
> 我们两点启示。
>
> （1）哪一家公司的语料的数量多、质量好，它的模型就会强于其他公司。
>
> （2）开源模型完全可以替代闭源模型，前提是训练语料要足够。
>
> 下面是 James Betker 的原文，大家仔细读读，看看是否认同他的观点。
>
> > 我在 OpenAI 工作已经快一年了。这段时间里，我训练了很多生成式 AI 模型，比任何人能想到的还要多。
> >
> > 每当我花了几个小时，观察和调整各种模型配置和参数时，有一件事让我印象深刻，那就是所有训练结果之间的相似性。
> >
> > 我越来越发现，**这些模型以令人难以置信的程度，向它们的语料集靠近。**
> >
> > 这表明在相同的语料集上训练足够长的时间，几乎每个具有足够权重和训练时间的模型都会收敛到同一点。足够大的扩散卷积网络会产生相同的结果。
> >
> > 这是一个令人惊讶的观察！
> >
> > 这意味着模型行为不是由架构、参数或优化器决定的。它由你的语料集决定，没有其他决定因素。其他一切因素都不过是为了有效计算以近似该语料集的手段。
> >
> > 当你谈论 Lambda、ChatGPT、Bard 或Claude 时，指的并不是它们的模型，而是它们的语料集。